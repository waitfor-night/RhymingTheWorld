# Introduction
The impressive capabilities of o1-like models have shown that using reinforcement learning to fine-tune language models is a highly promising approach with significant potential. However, likely due to the challenges posed by designing effective reward functions, most current efforts focus on testing this method in domains such as mathematical problem-solving or coding tasks. This project aims to introduce innovation by exploring how reinforcement learning can fine-tune LLMs to achieve semantically controlled and continuous rhyming generation.
<hr>